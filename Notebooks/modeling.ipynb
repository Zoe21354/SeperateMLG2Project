{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BUILDING MODELS**\n",
    "\n",
    "Having analysed, cleaned and split the dataset, mo\n",
    "\n",
    "    1. Model 1\n",
    "        A. Build Model\n",
    "        B. Predictions of the Model\n",
    "        C. Feature Importance from the Model\n",
    "        D. Create Pickle File\n",
    "    2. Model 2\n",
    "        A. Build Model\n",
    "        B. Predictions of the Model\n",
    "        C. Feature Importance from the Model\n",
    "        D. Create Pickle File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training and test data, we can move onto building the initial model.\n",
    "\n",
    "## **5. MODEL 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Build the Model\n",
    "A **Logistics Regression** model will be built using the train_data dataset and fit it to the model, whereas the test_data dataset will be used to predict the outcomes of the target attribute and compare the predictions to the actual answer to determin the accuracy of the model that was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create a Logistic Regression model and Fit the model with the training data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model1 \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[1;32m----> 3\u001b[0m model1\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Calculate the accuracy score for the predictions of the model\u001b[39;00m\n\u001b[0;32m      6\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m model1\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a Logistic Regression model and Fit the model with the training data\n",
    "model1 = LogisticRegression()\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the accuracy score for the predictions of the model\n",
    "test_predictions = model1.predict(X_test)\n",
    "print(f\"Accuracy Score for Predictions: {accuracy_score(y_test,test_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insight Gained:***\n",
    "\n",
    "    - The model shows it can accurately predict 77.24% of the Loan_Status values correctly.\n",
    "\n",
    "Cross validation will be used on the predictions generated by the model to check its validity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cross Validation model 1**\n",
    "Stratified K-Fold Cross Validation method is used. This variation of k-fold cross-validation is used when the target variable is imbalanced. It ensures that each fold is a good representative of the whole dataset. The average for all the iterated accuracy scores is calculated to determin the overall accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Stratified K-Fold Cross Validation\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "cross_val_predictions = cross_val_predict(model1, X, y, cv=skf)\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n",
    "i = 1\n",
    "scores = [] \n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    print('\\n{} of kfold {}'.format(i, kf.n_splits))\n",
    "    xtr, xvl = X.iloc[train_index], X.iloc[test_index]\n",
    "    ytr, yvl = y.iloc[train_index], y.iloc[test_index]\n",
    "    model = LogisticRegression(random_state=1)\n",
    "    model.fit(xtr, ytr)\n",
    "    pred_test = model.predict(xvl)\n",
    "    score = accuracy_score(yvl, pred_test)\n",
    "    scores.append(score)\n",
    "    print('accuracy_score:', score)\n",
    "    i += 1\n",
    "\n",
    "# Calculate the mean validation accuracy score\n",
    "mean_score = np.mean(scores)\n",
    "print(f\"\\nMean validation accuracy score: {mean_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insight Gained:***\n",
    "\n",
    "    - The difference between the two accuracy scores can be attributed to the fact that cross-validation provides a \n",
    "      more robust measure of the model’s performance.\n",
    "    - In cross-validation, the model is trained and tested on different subsets of the data, which helps to ensure \n",
    "      that the model’s performance is not overly dependent on the specific way the data was split into training and\n",
    "      test sets.\n",
    "    - The higher mean validation accuracy score suggests that the model’s performance may be slightly better than what\n",
    "      was observed on the initial test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Predictions of the Model\n",
    "All the predictions generated from the first model are stored in CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the data from the Logistic Regression model Predictions.csv file to prevent duplicate storage\n",
    "open('Log_Reg_Mod1_Predictions.csv', 'w').close()\n",
    "open('Log_Reg_Mod1_Cross_Validate_Predictions.csv', 'w').close()\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df = pd.DataFrame(test_predictions, columns=['Predictions'])\n",
    "predictions_df.index.names = ['Index']\n",
    "predictions_df.to_csv('Log_Reg_Mod1_Predictions.csv', mode='a', header=True)\n",
    "\n",
    "# Save the cross-validation predictions to CSV file\n",
    "cross_val_predictions_df = pd.DataFrame(pred_test, columns=['Cross Validation Predictions'])\n",
    "cross_val_predictions_df.index.names = ['Index']\n",
    "cross_val_predictions_df.to_csv('Log_Reg_Mod1_Cross_Validate_Predictions.csv', mode='a', header=True)\n",
    "\n",
    "# Save the mean validation accuracy score to the same CSV file\n",
    "mean_score_df = pd.DataFrame([mean_score], columns=['Mean Validation Accuracy Score'])\n",
    "mean_score_df.to_csv('Log_Reg_Mod1_Cross_Validate_Predictions.csv', mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Feature Importance from the Model\n",
    "Feature engineering transforms or combines raw data into a format that can be easily understood by machine learning models.\n",
    "Creates predictive model features, also known as a dimensions or variables, to generate model predictions.\n",
    "This highlights the most important patterns and relationships in the data, which then assists the machine learning model to learn from the data more effectively.\n",
    "\n",
    "#### **Feature 1: Total Income**\n",
    "*'Total_Income'* is the first feature that can be created. It is achieved through the addition of the *'Applicants_Income'* and the *'Coapplicant_Income'*. The Total_Income is then normalised to reduce the affects of the extreme values that could arise from the addition of the two attributes. A distribution chart is created to visually see the new feature and its distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Total_Income']=train_data['Applicant_Income']+train_data['Coapplicant_Income']\n",
    "test_data['Total_Income']=test_data['Applicant_Income']+test_data['Coapplicant_Income']\n",
    "\n",
    "#Distribution normalization\n",
    "sns.distplot(train_data['Total_Income'])\n",
    "plt.title('Distribution of Total Income')\n",
    "plt.xlabel('Total Income')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "train_data['Total_Income_Log']=np.log(train_data['Total_Income'])\n",
    "test_data['Total_Income_Log']=np.log(test_data['Total_Income'])\n",
    "\n",
    "sns.distplot(train_data['Total_Income_Log'])\n",
    "plt.title('Distribution of Total Income Log')\n",
    "plt.xlabel('Total Income Log')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Feature 2: Equated Monthly Installment (EMI)**\n",
    "The second feature we can create is an *'EMI'* attribute. It can be created by dividing the *‘Loan_Amount’* by the *‘Loan_Amount_Term’*. This feature gets the monthly payment amount for a loan, given the total loan amount and the term of the loan. Overall this will give an indication of the individuals monthly financial obligation towards the loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['EMI']=train_data['Loan_Amount']/train_data['Loan_Amount_Term']\n",
    "test_data['EMI'] = test_data['Loan_Amount']/test_data['Loan_Amount_Term']\n",
    "\n",
    "sns.distplot(train_data['EMI'])\n",
    "plt.title('Distribution of Equated Monthly Installments')\n",
    "plt.xlabel('Equated Monthly Installment')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Feature 3: Income After EMI**\n",
    "Lastly, a feature called \"Income After EMI\" can be created by dividing the ‘Loan_Amount’ by the ‘Loan_Amount_Term’ to get the monthly payment amount for a loan. This will give an indication of the individuals monthly financial obligation towards the loan. The 'EMI' feature is multiplied with 1000 to make the unit equal to the 'Total_Income' unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature 3: Income After EMI\n",
    "train_data['Income_After_EMI']=train_data['Total_Income']-(train_data['EMI']*1000)\n",
    "test_data['Income_After_EMI']=test_data['Total_Income']-(test_data['EMI']*1000)\n",
    "\n",
    "sns.distplot(train_data['Income_After_EMI'])\n",
    "plt.title('Distribution of Income After EMI')\n",
    "plt.xlabel('Income After EMI')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Remove all features that created the new features**\n",
    "The last step in the feature engineering section is to remove all the attributes used to create the new features. This is due to the high correlation between those old attributes and the new features. A Logistic regression model assumes that the attributes are not highly correlated. Therefore any excess noise in the datasets are removed. The new features are stored into a CSV file for use in the second model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.drop(['Applicant_Income','Coapplicant_Income','Loan_Amount','Loan_Amount_Term'],axis=1)\n",
    "test_data=test_data.drop(['Applicant_Income','Coapplicant_Income','Loan_Amount','Loan_Amount_Term'],axis=1)\n",
    "\n",
    "#Check to see if the attributes have been removed\n",
    "print(f\"Training Data Columns: {train_data.columns}\\n\")\n",
    "print(f\"Testing Data Columns:{test_data.columns}\\n\")\n",
    "\n",
    "# Store new Features in CSV files\n",
    "train_data.to_csv('Feature_Importance_train_data_NF_Model1.csv', index=False)\n",
    "test_data.to_csv('Feature_Importance_test_data_NF_Model1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Create Pickle File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a pickle file\n",
    "with open('Model_1.pkl', 'wb') as f:\n",
    "    pickle.dump(model1, f)\n",
    "\n",
    "#View data in the Model_1.pkl file\n",
    "with open('Model_1.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(f\"Model 1 pickle file data: {data}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================\n",
    "# **5. MODEL 2**\n",
    "\n",
    "## A. Build the Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
