{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Project Details:**\n",
    "**Title:** Guided Project\n",
    "\n",
    "**Due Date:** 9 May 2024\n",
    "\n",
    "**Contributors:** Marcus Mahlatjie (577296) and Zoë Treutens (577989)\n",
    "\n",
    "**GitHub Link:** https://github.com/Zoe21354/SeperateMLG2Project.git\n",
    "\n",
    "**Note to Lecturer:**\n",
    "To whom ever is marking this project please kindly note that the other students assigned to our group (F) have not contributed to this work and is solely done by the above mentioned students. This is due to...\n",
    "\n",
    "==================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Streamlining BC Finance’s Home Loan Eligibility Process**\n",
    "BC Finance Company provides financial services across all home loan categories. Offering services to clients in urban, semi-urban, and rural areas, the organization has many facets. The organization currently uses an **ineffective manual** procedure to validate customers' eligibility. The procedure entails the client submitting an application for a home loan by answering questions and supplying personal information. These responses must then go through a lengthy validation process and this can be a problem for handling multiple applications leading to decreased customer satisfaction, manual errors and **lengthy application times** which could lead to customers seeking other financial institutes to provide financial services for their needs.\n",
    "\n",
    "The organization is working to create an **automated system** that can accurately determine a customer's eligibility for a home loan in real time in order to address this problem. To ascertain if a customer is eligible for a loan, this system will examine a number of customer variables, including gender, marital status, education, number of dependents, income, loan amount, credit history, and others.\n",
    "\n",
    "The principal aim is to divide clients into discrete categories according to their loan quantum eligibility. By doing this, BC Finance hopes to **efficiently and successfully target** these consumer segments and provide them with loan products and services that are customized to their unique requirements and preferences. BC Finance hopes to improve client happiness, reduce manual errors, and streamline its lending procedures for long-term profitability and growth by putting in place an automated loan qualifying system.\n",
    "\n",
    "This notebook will take the following structure:\n",
    "\n",
    "    1. Prepare Data (Data Analysis)\n",
    "        A. Dataset Analysis\n",
    "        B. Univariate Analysis\n",
    "        C. Bi-variate Analysis\n",
    "    2. Hypotheses\n",
    "    3. Preprocess Data (Data Cleaning)\n",
    "        A. Handling missing values\n",
    "        B. Removing duplicates\n",
    "        C. Outlier value Handling\n",
    "    4. Split Dataset\n",
    "    5. Model 1\n",
    "        A. Build Model\n",
    "        B. Predictions of the Model\n",
    "        C. Feature Importance from the Model\n",
    "        D. Create Pickle File\n",
    "    6. Model 2\n",
    "        A. Build Model\n",
    "        B. Predictions of the Model\n",
    "        C. Feature Importance from the Model\n",
    "        D. Create Pickle File\n",
    "    10. Validate the Model\n",
    "    11. Web Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Prepare Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before any analysis can take place, certain libraries in python need to be imported to perform different functions and make various features available for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import csv                                              # Read and Write to CSV files\n",
    "import pandas as pd                                     # Manipulation and analysis of data\n",
    "import numpy as np                                      # Mathematical operations\n",
    "import matplotlib.pyplot as plt                         # Matplotlib and Seaborn is used to create visual graphs\n",
    "import seaborn as sns                                   \n",
    "from sklearn.model_selection import train_test_split    # Splits the raw_data into two sets of data\n",
    "import warnings                                         # Ignores any future warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSV files named raw_data and validation_data are read so that the unclean data contained in these files can be analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'raw_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[581], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Read Unclean CSV Files\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m raw_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m raw_data_copy \u001b[38;5;241m=\u001b[39m raw_data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      5\u001b[0m validation_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'raw_data.csv'"
     ]
    }
   ],
   "source": [
    "# Read Unclean CSV Files\n",
    "raw_data = pd.read_csv(\"raw_data.csv\")\n",
    "raw_data_copy = raw_data.copy()\n",
    "\n",
    "validation_data = pd.read_csv(\"validation.csv\")\n",
    "validation_data_copy = validation_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA ANALYSIS PROCESSES**\n",
    "\n",
    "Performing data analysis on unclean data is essential, as it will provide pertinant information regarding the data we are to use. Although, data cleaning is an essential step in the data analysis process, understanding your data before you clean it can make the process more efficient and effective.\n",
    "\n",
    "***Understanding the Data*** \n",
    "It allows you to understand the nature and structure of your data. You can identify the types of variables you have, their distribution, and how they relate to each other.\n",
    "\n",
    "***Identifying Errors and Anomalies*** \n",
    "Unclean data can contain errors, outliers, or anomalies that need to be addressed. By analysing the data first, you can identify these issues and plan how to handle them during the cleaning process.\n",
    "\n",
    "***Determining Cleaning Strategies*** \n",
    "Not all data requires the same cleaning procedures. Analysing the data can help you determine the most appropriate cleaning strategies for your specific dataset.\n",
    "\n",
    "***Preserving Valuable Information*** \n",
    "Sometimes, what might initially appear as an error or outlier could actually be a valuable piece of information. Analysing the data before cleaning ensures that you don’t inadvertently remove these insights.\n",
    "\n",
    "***Improving Model Accuracy***\n",
    "Unclean data can lead to inaccurate models. By analysing and cleaning your data, you can improve the accuracy of your subsequent models.\n",
    "\n",
    "## A. Dataset Analysis\n",
    "### *Dataset Attributes:*\n",
    "Each attribute in the dataset represents a different variable. Understanding these attributes helps you understand the variables you’re working with, what they represent, and how they might relate to your research question or problem statement.\n",
    "\n",
    "  - Feature Variable (Independent variables) are variables that stand alone and are not changed by other variables that are being \n",
    "    measured. They are denoted as X in ML algorithms.\n",
    "  - Target Variables (Dependent variables) are the variables that are to be predicted. It is often denoted as Y in ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Raw Data Columns:\\n{raw_data_copy.columns}\\n\")\n",
    "print(f\"Validation Data Columns:\\n{validation_data_copy.columns}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insight Gained:***\n",
    "  - In both datasets, the attribute names are written inconsistently. Some attributes have underscores between each word ie. \n",
    "    Loan_ID and other attributes use PascalCase i.e ApplicantIncome. This will need to be standardized in the data processing \n",
    "    section.\n",
    "  - For both datasets, there are 12 feature variables but only the \"raw_data\" dataset has 1 target variable.\n",
    "  - The target variable in the raw_data dataset is the Loan_Status attribute.\n",
    "  - This variable will be predicted using models for the \"validation_data\" dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Dataset Datatypes:*\n",
    "Attributes can have different data types, such as numerical, categorical, or ordinal. Knowing the data type of each attribute is important because it determines what kind of statistical analysis or data processing is appropriate. Learning the different datatypes for each attribute in both of the datasets will provide insight into the consistance of the datattypes for each specific attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Raw Dataset Datatypes:\\n{raw_data_copy.dtypes}\\n\")\n",
    "print(f\"Validation Dataset Datatypes:\\n{validation_data_copy.dtypes}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insight Gained:***\n",
    "  - There is a discrepancy between the two datasets: the \"Dependents\" attribute is of datatype float64 in the \"raw_data\" dataset \n",
    "    but of datatype object is seen in the \"validation_data\" file.\n",
    "  - This could lead to potentially issues when modeling, as the model might be expecting the same datatype for a given attribute.\n",
    "  - This discrepancy will need to be fixed in the data processing section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Dataset Shape:*\n",
    "Knowing the number of rows in your datasets provides you with an idea of the volume of the data available to you. More rows mean more data, which can lead to more robust and reliable models. However, it can also mean more computational resources and time required for processing. On the other hand knowing the number of columns in the dataset informs the user on the number of features (or variables) available. Overall the analysis of the shape of the dataset can help in assessing the quality of the data.\n",
    "\n",
    "For example, if you have many rows but few columns, you might not have enough features to build a good model. Conversely, having a large number of columns compared to rows could lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Raw Data Shape: {raw_data_copy.shape}\\n\")\n",
    "print(f\"Validation Data Shape:{validation_data_copy.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insight Gained:***\n",
    "- Raw Data Shape: 614 rows and 13 columns\n",
    "- Validation Data Shape: 367 rows and 12 columns\n",
    "- There is exactly 1 less column in the \"validation_data\" file which is the target attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Univariate Analysis\n",
    "Univariate analysis is the process of analysising individual (one variable) at a time. This is the most basic type of data analysis to  finds patterns in the data.\n",
    "\n",
    "Analyzing univariate data involves examining the frequency of data in the dataset. In order to do this the count for each category in the attribute is found. Following this the data is normalized to get the proportion of the different categories through the division of the count by the total number of values. Lastly a bar chart is plotted to visualise the data.\n",
    "\n",
    "### *Dependent (Target) Attribute:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = raw_data_copy['Loan_Status'].value_counts(normalize = True)\n",
    "plt.figure(figsize=(7, 7))\n",
    "chart = count.plot.bar(title = 'Loan_Status', xlabel = 'Categories', ylabel = 'Frequency')\n",
    "for i, v in enumerate(count):\n",
    "    chart.text(i, v + 0.01, str(round(v, 2)), ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insight Gained:***\n",
    "- 0.69 or 69% of the people were approved for a loan (i.e Loan_Status = Yes)\n",
    "- 0.31 or 31% of the people were not approved for a loan (i.e Loan_Status = No)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Independent Attributes (Categorical):*\n",
    "Categorical data is a type of data that is qualitative and has no numerical values. It can be divided into categories but cannot be ordered or measured. For examples, \n",
    "    \n",
    "- Colour category can include: red, blue, or green\n",
    "- Gender category can include: male or female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender Attribute\n",
    "count = raw_data_copy['Gender'].value_counts(normalize = True)\n",
    "plt.figure(figsize=(7, 7))\n",
    "chart = count.plot.bar(title = 'Gender', xlabel = 'Categories', ylabel = 'Frequency')\n",
    "for i, v in enumerate(count):\n",
    "    chart.text(i, v + 0.01, str(round(v, 2)), ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insight Gained:***\n",
    "- 0.81 or 81% of the people are male (i.e Gender = Male)\n",
    "- 0.19 or 19% of the people are female (i.e Gender = Female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Married Attribute\n",
    "count = raw_data_copy['Married'].value_counts(normalize = True)\n",
    "plt.figure(figsize=(7, 7))\n",
    "chart = count.plot.bar(title='Married', xlabel = 'Categories', ylabel = 'Frequency')\n",
    "for i, v in enumerate(count):\n",
    "    chart.text(i, v + 0.01, str(round(v, 2)), ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insight Gained:***\n",
    "- 0.65 or 65% of the people were Married (i.e Married = Yes)\n",
    "- 0.35 or 35% of the people were not Married (i.e Married = No)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self_Employed Attribute\n",
    "count = raw_data_copy['Self_Employed'].value_counts(normalize = True)\n",
    "plt.figure(figsize=(7, 7))\n",
    "chart = count.plot.bar(title='Self_Employed', xlabel = 'Categories', ylabel = 'Frequency')\n",
    "for i, v in enumerate(count):\n",
    "    chart.text(i, v + 0.01, str(round(v, 2)), ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insight Gained:***\n",
    "- 0.14 or 14% of the people are self-employed (i.e Self_Employed = Yes)\n",
    "- 0.86 or 86% of the people are not self-employed (i.e Married = No)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit_History Attribute\n",
    "count = raw_data_copy['Credit_History'].value_counts(normalize = True)\n",
    "plt.figure(figsize=(7, 7))\n",
    "chart = count.plot.bar(title='Credit_History', xlabel = 'Categories', ylabel = 'Frequency')\n",
    "for i, v in enumerate(count):\n",
    "    chart.text(i, v + 0.01, str(round(v, 2)), ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insight Gained:***\n",
    "- 0.84 or 84% of the people have a credit history (i.e Credit_History = 1)\n",
    "- 0.16 or 16% of the people don't have a credit history (i.e Credit_History = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Independent Attributes (Ordinal):*\n",
    "Ordinal data have a clear ordering or hierarchy in the categories. For example, customer satisfaction ratings can include: unsatisfied, neutral, or satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependents Attribute\n",
    "count = raw_data_copy['Dependents'].value_counts('normalize = True')\n",
    "plt.figure(figsize=(7, 7))\n",
    "chart = count.plot.bar(title='Dependents', xlabel = 'Categories', ylabel = 'Frequency')\n",
    "for i, v in enumerate(count):\n",
    "    chart.text(i, v + 0.01, str(round(v, 2)), ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insight Gained:***\n",
    "- 0.58 or 58% of the people don't have Dependent (i.e Dependents = 0)\n",
    "- 0.17 or 17% of the people has only one Dependent (i.e Dependents = 1)\n",
    "- 0.17 or 17% of the people has two Dependents (i.e Dependents = 2)\n",
    "- 0.09 or 9% of the people has three or more Dependents (i.e Dependents = 3+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Education Attribute\n",
    "count =raw_data_copy['Education'].value_counts('normalize = True')\n",
    "plt.figure(figsize=(7, 7))\n",
    "chart = count.plot.bar(title='Education', xlabel = 'Categories', ylabel = 'Frequency')\n",
    "for i, v in enumerate(count):\n",
    "    chart.text(i, v + 0.01, str(round(v, 2)), ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insight Gained:***\n",
    "- 0.78 or 78% of the people have graduated (i.e Education = Graduate)\n",
    "- 0.22 or 22% of the people have not graduated (i.e Education = Not Graduate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Property_Area Attribute\n",
    "count = raw_data_copy['Property_Area'].value_counts('normalize=True')\n",
    "plt.figure(figsize=(7, 9))\n",
    "chart = count.plot.bar(title='Property_Area', xlabel = 'Categories', ylabel = 'Frequency')\n",
    "for i, v in enumerate(count):\n",
    "    chart.text(i, v + 0.01, str(round(v, 2)), ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insight Gained:***\n",
    "- 0.38 or 38% of the people are located in the semi-urban area (i.e Property_Area = Semiurban)\n",
    "- 0.33 or 33% of the people are located in the urban area (i.e Property_Area = Urban)\n",
    "- 0.29 or 29% of the people are located in the rural area(i.e Property_Area = Rural)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Independent Attributes (Nominal)*\n",
    "Nominal data does not have any kind of order or hierarchy but rather each category are different from each other. For example, the different breeds of dogs (Labrador, Beagle, Poodle) constitute nominal data because there is no inherent order among them.\n",
    "\n",
    "A distribution chart is used to visualise the distribution of the values in the attributes ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount_Term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "raw_data_copy.dropna()  # Drop missing data in the attribute's data\n",
    "sns.distplot(raw_data_copy['ApplicantIncome'])\n",
    "plt.title('Distribution of Applicant Income')\n",
    "plt.xlabel('Applicant Income')\n",
    "plt.ylabel('Density')\n",
    "plt.subplot(122)\n",
    "boxplot = raw_data_copy['ApplicantIncome'].plot.box()\n",
    "boxplot.set_title('Box Plot of Applicant Income')\n",
    "boxplot.set_xlabel('Density')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(2, figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "raw_data_copy.dropna()\n",
    "sns.distplot(raw_data_copy['CoapplicantIncome'])\n",
    "plt.title('Distribution of Coapplicant Income')\n",
    "plt.xlabel('Coapplicant Income')\n",
    "plt.ylabel('Density')\n",
    "plt.subplot(122)\n",
    "boxplot = raw_data_copy['CoapplicantIncome'].plot.box()\n",
    "boxplot.set_title('Box Plot of Coapplicant Income')\n",
    "boxplot.set_xlabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insight Gained:***\n",
    "  - Both the distribution charts of the ApplicantIncome and CoapplicantIncome show a left-skewed distribution that indicates a \n",
    "    majority of the applicants have lower incomes.\n",
    "  - This pattern reflects income inequality within the applicant pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3, figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "raw_data_copy.dropna() \n",
    "sns.distplot(raw_data_copy['LoanAmount'])\n",
    "plt.title('Distribution of Loan Amount')\n",
    "plt.xlabel('Loan Amount')\n",
    "plt.ylabel('Density')\n",
    "plt.subplot(122)\n",
    "boxplot =raw_data_copy['LoanAmount'].plot.box()\n",
    "boxplot.set_title('Box Plot of Loan Amount')\n",
    "boxplot.set_xlabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insight Gained:***\n",
    "- Overall the distribution of the data is fairly normal.\n",
    "- There are outliers in this attribute which could negatively impact the mean and distribution of the data \n",
    "- These outliers will be treated in the data cleaning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(4, figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "raw_data_copy.dropna() \n",
    "sns.distplot(raw_data_copy['Loan_Amount_Term'])\n",
    "plt.title('Distribution of Loan Amount Term')\n",
    "plt.xlabel('Loan Amount Term')\n",
    "plt.ylabel('Density')\n",
    "plt.subplot(122)\n",
    "boxplot =raw_data_copy['Loan_Amount_Term'].plot.box()\n",
    "boxplot.set_title('Box Plot of Loan Amount Term')\n",
    "boxplot.set_xlabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insight Gained:***\n",
    "- The peak around 360 indicates a standard loan term.\n",
    "- Smaller peaks at lower values show that shorter loan terms are less common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Bi-variate Analysis\n",
    "\n",
    "When there are two variables in the data it is called bi-variate analysis. The data is analyzed to find the relationship between the dependent and independent variables. Stacked bar graphs can be utilised to view the correlation between the coefficients.\n",
    "\n",
    "The graphs created below will display how the Dependent Attribute ‘Loan_Status’ is distributed within each Independent Attribute, regardless of how many observations there are.\n",
    "\n",
    "### *Categorical Independent Variables and Dependent Variable LoanAmount:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loan_Status vs Gender\n",
    "gender_table = pd.crosstab(raw_data_copy['Gender'], raw_data_copy['Loan_Status'])\n",
    "gender_table.div(gender_table.sum(1).astype(float),axis=0).plot(kind='bar', stacked=True)\n",
    "plt.title('Loan Status by Gender Category')\n",
    "plt.xlabel('Gender Categories')\n",
    "plt.ylabel('Loan Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insight Gained:***\n",
    "  - The proportion of the loan status ‘Yes’ is slightly higher for males, indicating a marginally higher approval rate compared\n",
    "    to females.\n",
    "  - For both genders, the majority of the loan status is ‘Yes’, suggesting that most applicants in the dataset were approved for \n",
    "    a loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loan_Status vs Married\n",
    "married_table = pd.crosstab(raw_data_copy['Married'], raw_data_copy['Loan_Status'])\n",
    "married_table.div(married_table.sum(1).astype(float),axis=0).plot(kind='bar', stacked=True)\n",
    "plt.title('Loan Status by Marriage Category')\n",
    "plt.xlabel('Married Categories')\n",
    "plt.ylabel('Loan Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insight Gained:***\n",
    "  - The ‘Yes’ category shows a higher proportion for the loan status ‘Yes’, suggesting that married individuals may have a better \n",
    "    chance of loan approval.\n",
    "  - Conversely, the ‘No’ category has a higher proportion for the loan status ‘No’, indicating that unmarried individuals may \n",
    "    face more rejections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loan_Status vs Self_Employed\n",
    "self_employed_table = pd.crosstab(raw_data_copy['Self_Employed'], raw_data_copy['Loan_Status'])\n",
    "self_employed_table.div(self_employed_table.sum(1).astype(float),axis=0).plot(kind='bar', stacked=True)\n",
    "plt.title('Loan Status by Self Employment Category')\n",
    "plt.xlabel('Self Employment Categories')\n",
    "plt.ylabel('Loan Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insight Gained:***\n",
    "  - The ‘Yes’ loan status is present in both self-employment categories, but there is a slightly larger proportion of \n",
    "    approvals for individuals who are not self-employed (‘No’) compared to those who are self-employed (‘Yes’).\n",
    "  - Self-Employment Impact: The graph suggests that being self-employed might have a slight impact on loan approval rates, \n",
    "    although the difference is not substantial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loan_Status vs Credit_History\n",
    "credit_history_table = pd.crosstab(raw_data_copy['Credit_History'], raw_data_copy['Loan_Status'])\n",
    "credit_history_table.div(credit_history_table.sum(1).astype(float),axis=0).plot(kind='bar', stacked=True)\n",
    "plt.title('Loan Status by Credit History Category')\n",
    "plt.xlabel('Credit History Categories')\n",
    "plt.ylabel('Loan Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insight Gained:***\n",
    "  - Individuals in Credit History Category ‘1’ have a higher proportion of getting approval for a loan, indicating a \n",
    "    positive correlation between a good credit history and loan approval.\n",
    "  - Category ‘0’ has a higher proportion of being rejected for a loan approval, suggesting that a poor credit history is \n",
    "    associated with higher loan rejections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Ordinal Independent Variables and Dependent Variable LoanAmount*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loan_Status vs Dependents\n",
    "dependents_table = pd.crosstab(raw_data_copy['Dependents'], raw_data_copy['Loan_Status'])\n",
    "dependents_table.div(dependents_table.sum(1).astype(float),axis=0).plot(kind='bar', stacked=True)\n",
    "plt.title('Loan Status by Dependent Category')\n",
    "plt.xlabel('Dependent Categories')\n",
    "plt.ylabel('Loan Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insight Gained:***\n",
    "  - The ‘Yes’ loan status is present across all dependent categories, but there is a trend where the proportion of \n",
    "    approvals decreases as the number of dependents increases.\n",
    "  - The graph suggests that having more dependents might negatively impact the rate of loan approval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loan_Status vs Education\n",
    "education_table = pd.crosstab(raw_data_copy['Education'], raw_data_copy['Loan_Status'])\n",
    "education_table.div(education_table.sum(1).astype(float),axis=0).plot(kind='bar', stacked=True)\n",
    "plt.title('Loan Status by Education Category')\n",
    "plt.xlabel('Education Categories')\n",
    "plt.ylabel('Loan Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Insight Gained:***\n",
    "  - A larger proportion of graduates have their loans approved (‘Y’) compared to non-graduates who have a higher \n",
    "    proportion of being rejected (‘N’)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loan_Status vs Property Area\n",
    "property_area_table = pd.crosstab(raw_data_copy['Property_Area'], raw_data_copy['Loan_Status'])\n",
    "property_area_table.div(property_area_table.sum(1).astype(float),axis=0).plot(kind='bar', stacked=True)\n",
    "plt.title('Loan Status by Property Area Category')\n",
    "plt.xlabel('Property Area Categories')\n",
    "plt.ylabel('Loan Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Insight Gained:***\n",
    "  - The Semiurban areas have the highest proportion of approved loans (‘Y’), suggesting a favorable outcome for loan \n",
    "    applicants in these areas.\n",
    "  - The Rural area has the lowest proportion of approved loans, indicating potential challenges or stricter criteria for \n",
    "    loan approval.\n",
    "  - Urban Observations: The Urban area has a moderate proportion of approved loans, falling between the Rural and \n",
    "    Semiurban areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Numerical Independent Variables and Dependent Variable LoanAmount*\n",
    "The purpose of this section is to provide insight into how the income levels (both individually and combined with co-applicants) relate to the likelihood of a loan being approved. In order to determine the impact of the income on the Loan_Status, the mean income is calculated to determine who's loans were approved vs who's were not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loan_Status vs Applicant_Income\n",
    "raw_data_copy.groupby('Loan_Status')['ApplicantIncome'].mean().plot.bar()\n",
    "plt.title('Average Applicant Income by Loan Status')\n",
    "plt.xlabel('Loan Status')\n",
    "plt.ylabel('Average Applicant Income')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ApplicantIncome is categorized in loans within each income bracket. This will access whether different income levels, when the applicant income and the co-applicant income are added together, will influence the Loan approval rate.\n",
    "\n",
    "Binning will transform the continuous numerical variables into discrete categorical ‘bins’. Income brackets such as \"Low\", \"Average\", \"Above Average\", and \"High\" are used to provide a qualitative understanding of the ranges in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loan_Status vs Total Income\n",
    "# Combine the applicant income and co-applicant income together\n",
    "raw_data_copy['Total_Income']=raw_data_copy['ApplicantIncome']+raw_data_copy['CoapplicantIncome']\n",
    "\n",
    "# Calculate the bin values\n",
    "low = raw_data_copy['ApplicantIncome'].quantile(0.25) # 25th percentile\n",
    "average = raw_data_copy['ApplicantIncome'].quantile(0.50) # 50th percentile\n",
    "above_average = raw_data_copy['ApplicantIncome'].quantile(0.75) # 75th percentile\n",
    "high = 81000\n",
    "\n",
    "bins = [0, low, average, above_average, high]\n",
    "group=['Low','Average','Above Average','High']\n",
    "\n",
    "raw_data_copy['Total_Income_bin']=pd.cut(raw_data_copy['Total_Income'],bins,labels=group)\n",
    "Total_Income_bin=pd.crosstab(raw_data_copy['Total_Income_bin'],raw_data_copy['Loan_Status'])\n",
    "Total_Income_bin.div(Total_Income_bin.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True)\n",
    "plt.title('Percentage of Total Income Per Income Bracket')\n",
    "plt.xlabel('Total Income')\n",
    "plt.ylabel('Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Insight Gained:***\n",
    "  - The proportion of loans approved for applicants with low total income is significantly lower than for other income groups.\n",
    "  - Applicants with average, high, and very high total income have a greater proportion of loan approvals.\n",
    "  - The chart suggests that total income level may impact the likelihood of loan approval.\n",
    "  - This analysis indicates that higher income levels are associated with better chances of loan approval, highlighting the \n",
    "    importance of income in the loan decision process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loan amount is categorized in loans within each loan bracket. This will access whether different loan amounts will influence the Loan approval rate.\n",
    "\n",
    "Loan brackets such as \"Low\", \"Average\", and \"High\" are used to provide a qualitative understanding of the ranges in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loan_Status vs Loan Amount\n",
    "# Calculate the bin values\n",
    "low = raw_data_copy['LoanAmount'].quantile(0.333) # 33.3th percentile\n",
    "average = raw_data_copy['LoanAmount'].quantile(0.666) # 66.6th percentile\n",
    "high = 700\n",
    "\n",
    "bins = [0, low, average, high]\n",
    "group=['Low','Average','High']\n",
    "\n",
    "raw_data_copy['Loan_Amount_bin']=pd.cut(raw_data_copy['LoanAmount'],bins,labels=group)\n",
    "Total_Income_bin=pd.crosstab(raw_data_copy['Loan_Amount_bin'],raw_data_copy['Loan_Status'])\n",
    "Total_Income_bin.div(Total_Income_bin.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True)\n",
    "plt.title('Percentage of Loan Amount Per Loan Bracket')\n",
    "plt.xlabel('Loan Amount')\n",
    "plt.ylabel('Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Insight Gained:***\n",
    "  - Low and Average Loan Amounts: The proportion of approved loans is higher for these categories, indicating a greater \n",
    "    likelihood of approval for smaller loan amounts.\n",
    "  - High Loan Amount: The proportion of approved loans is lower for this category, suggesting that larger loan amounts may have a \n",
    "    reduced chance of approval.\n",
    "  - Therefore it can be said that loans with lower amounts are more likely to be approved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must now remove all bins created to avoid redundancy and reduce data complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all bins created:\n",
    "raw_data_copy=raw_data_copy.drop(['Loan_Amount_bin','Total_Income_bin','Total_Income'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmaps are a powerful visualization tool that display data through variations in coloring. When used with datasets, the colors correspond to the values in each cell of a matrix. Darker colors typically represent higher correlation values (either positive or negative), while lighter colors represent lower correlation values or no correlation. This allows for an intuitive, visual interpretation of how different numerical attributes relate to each other in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = validation_data_copy.select_dtypes(include=[np.number])\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(numeric_cols.corr(), annot=True, cmap='PuRd')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Insight Gained:***\n",
    "\n",
    "- Moderate Correlation: \n",
    "    ApplicantIncome and LoanAmount have a moderate positive correlation, suggesting that as applicant income increases, the \n",
    "    loan amount tends to increase as well.\n",
    "\n",
    "- Credit History Impact: \n",
    "    The moderate positive correlation between Credit_History and Loan_Status indicates that applicants with a good credit \n",
    "    history are more likely to have their loans approved.\n",
    "\n",
    "- Coapplicant Contribution: \n",
    "    While there is a positive correlation between LoanAmount and CoapplicantIncome, it is relatively weak, implying that co-\n",
    "    applicant income has a lesser impact on the loan amount compared to the primary applicant’s income.\n",
    "\n",
    "- Overall, the heatmap suggests that both income and credit history play significant roles in loan amount determination and \n",
    "    approval. The weaker correlation for CoapplicantIncome may indicate that lenders prioritize the primary applicant’s financial \n",
    "    status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having now seen how the different attributes impact the outcome in our datasets, several hypotheses can be drawn from the results.\n",
    "\n",
    "===================================================================================\n",
    "# **2. Hypotheses**\n",
    "\n",
    "The aim of this project is to use machine learning to transform BC Finance's loan approval process. BC Finance seeks to mitigate the inefficiencies linked to manual validation, including longer application periods, higher error rates, and lower customer satisfaction, by automating the real-time eligibility evaluation process. By using this automated approach, BC Finance hopes to improve resource allocation, boost operational efficiency, and ultimately become more competitive in the financial market.\n",
    "\n",
    "The _'prepare_data.py'_ file is essential in this situation as it organizes several data pretreatment and exploratory analysis activities. By carefully going over the dataset, which includes factors like gender, marital status, income levels, credit history, and property location, this script reveals important information that serves as the foundation for the phases of model construction and hypothesis formulation that follow.\n",
    "\n",
    "- __Hypothesis 1__: The likelihood of loan approval is positively impacted by having a good credit history. \n",
    "    - Justification: Bi-variate analysis shows a moderately positive correlation between Credit_History and Loan_Status, indicating that applicants with a good credit history are more likely to have their loans approved. \n",
    "\n",
    "- __Hypothesis 2__: Loan amounts in the low to average range are more likely to be approved \n",
    "    than high loan amounts. \n",
    "    - Justification: Bi-variate analysis shows that the proportion of approved loans is higher for low and average loan amounts, indicating a greater likelihood of approval for smaller loan amounts. \n",
    "\n",
    "- __Hypothesis 3__: An applicant's marital status may have an impact on loan approval rates. \n",
    "    - Justification: Bi-variate analysis displays differences in loan approval rates for married individuals compared to unmarried individuals.\n",
    "\n",
    "- __Hypothesis 4__: The type of property—rural, semi-urban, or urban—may affect the likelihood of a loan being approved. \n",
    "    - Justification: variable property areas have variable loan approval rates, as shown by univariate analysis. As an illustration, the percentage of loans that are authorized is higher in semi-urban areas than in urban and rural areas.\n",
    "\n",
    "- __Hypothesis 5__: There is a positive correlation between income levels and loan acceptance rates. \n",
    "    - Justification: According to univariate research, applicants with higher earnings typically receive a higher percentage of loan approvals. This implies that judgments about loan approval may be significantly influenced by an individual's income level.\n",
    "\n",
    "==================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. PREPROCESS THE DATA**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the analysis process, it can be seen that the data required cleaning. The most important step is to import the necessary libraries and read in the relevant CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                     # Manipulation and analysis of data\n",
    "import numpy as np                                      # Mathematical operations\n",
    "from scipy import stats                                 # Statistical functions\n",
    "import matplotlib.pyplot as plt                         # Matplotlib and Seaborn are used to create visual graphs\n",
    "import seaborn as sns   \n",
    "\n",
    "import warnings                                        \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "raw_data = pd.read_csv('raw_data.csv')\n",
    "raw_data_copy = raw_data.copy()\n",
    "\n",
    "validation_data = pd.read_csv('validation.csv')\n",
    "validation_data_copy = validation_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Attribute Name Standardization Process for both dataset\n",
    "Based on the insight gained from analysing the datasets, it was seen that the attribute names were inconsistant therefore the attribute require a renaming. Using a dictionary, the attribute names will be replaced with a standardised naming convention of Capitalised first letters and underscores ( _ ) between each word to join them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map the old column names to the new ones to format the column names\n",
    "raw_column_name_mapping = {\n",
    "    'Loan_ID': 'Loan_ID',\n",
    "    'Gender': 'Gender',\n",
    "    'Married': 'Married',\n",
    "    'Dependents': 'Dependents',\n",
    "    'Education': 'Education',\n",
    "    'Self_Employed': 'Self_Employed',\n",
    "    'ApplicantIncome': 'Applicant_Income',\n",
    "    'CoapplicantIncome': 'Coapplicant_Income',\n",
    "    'LoanAmount': 'Loan_Amount',\n",
    "    'Loan_Amount_Term': 'Loan_Amount_Term',\n",
    "    'Credit_History': 'Credit_History',\n",
    "    'Property_Area': 'Property_Area',\n",
    "    'Loan_Status': 'Loan_Status'\n",
    "}\n",
    "\n",
    "validation_column_name_mapping = {\n",
    "    'Loan_ID': 'Loan_ID',\n",
    "    'Gender': 'Gender',\n",
    "    'Married': 'Married',\n",
    "    'Dependents': 'Dependents',\n",
    "    'Education': 'Education',\n",
    "    'Self_Employed': 'Self_Employed',\n",
    "    'ApplicantIncome': 'Applicant_Income',\n",
    "    'CoapplicantIncome': 'Coapplicant_Income',\n",
    "    'LoanAmount': 'Loan_Amount',\n",
    "    'Loan_Amount_Term': 'Loan_Amount_Term',\n",
    "    'Credit_History': 'Credit_History',\n",
    "    'Property_Area': 'Property_Area'\n",
    "}\n",
    "\n",
    "raw_data_copy.rename(columns=raw_column_name_mapping, inplace=True)\n",
    "validation_data_copy.rename(columns=validation_column_name_mapping, inplace=True)\n",
    "\n",
    "#Check the changes made to the columns\n",
    "print(f\"Raw Data Columns:\\n{raw_data_copy.columns}\\n\")\n",
    "print(f\"Validation Data Columns:\\n{validation_data_copy.columns}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Checking for missing values in both datasets\n",
    "Ensuring that the datasets have no missing values is essencial because missing values can significantly impact the quality and performance of the machine learning models. Missing data can lead to biased or incorrect results, reduce the statistical power of the model, and make the data harder to interpret. Therefore, it’s crucial to handle missing values appropriately, either by filling them in using a suitable method (like mean, median, or mode imputation), or by removing the instances or features with missing values, depending on the nature and amount of the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of Missing Values in raw_data_copy:\\n{raw_data_copy.isnull().sum()}\\n\")\n",
    "print(f\"Number of Missing Values in validation_data_copy:\\n{raw_data_copy.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above, it can be seen that there are missing values in the following attributes for both data sets:\n",
    "\n",
    "- Gender\n",
    "- Married\n",
    "- Dependents\n",
    "- Self_Employed\n",
    "- LoanAmount\n",
    "- Loan_Amount_Term\n",
    "- Credit_History\n",
    "\n",
    "In order to fill in the missing values the attributes need to be split into **Categorical** and **Numerical** attributes.\n",
    "\n",
    "#### ***Categorical Attributes***\n",
    "The *mode* of all the values in the attribute to fill in the missing data values.\n",
    "For example:\n",
    "\n",
    "- Gender (Male or Female)\n",
    "- Married (Yes or No)\n",
    "- Dependents (0, 1, 2, or 3+)\n",
    "- Self_Employed (Yes or No)\n",
    "- Credit_History (1 or 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_copy['Gender'].fillna(raw_data_copy['Gender'].mode()[0],inplace=True)\n",
    "raw_data_copy['Married'].fillna(raw_data_copy['Married'].mode()[0],inplace=True)\n",
    "raw_data_copy['Dependents'].fillna(raw_data_copy['Dependents'].mode()[0],inplace=True)\n",
    "raw_data_copy['Self_Employed'].fillna(raw_data_copy['Self_Employed'].mode()[0],inplace=True)\n",
    "raw_data_copy['Credit_History'].fillna(raw_data_copy['Credit_History'].mode()[0],inplace=True)\n",
    "\n",
    "validation_data_copy['Gender'].fillna(validation_data_copy['Gender'].mode()[0],inplace=True)\n",
    "validation_data_copy['Married'].fillna(validation_data_copy['Married'].mode()[0],inplace=True)\n",
    "validation_data_copy['Dependents'].fillna(validation_data_copy['Dependents'].mode()[0],inplace=True)\n",
    "validation_data_copy['Self_Employed'].fillna(validation_data_copy['Self_Employed'].mode()[0],inplace=True)\n",
    "validation_data_copy['Credit_History'].fillna(validation_data_copy['Credit_History'].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Numerical Attributes***\n",
    "Either the *mean* or *median* of the values is used to fill in the missing data values. In the case of our dataset, median is used instead of mean due to the outliers in the attribute data which could negatively impact the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_copy['Loan_Amount'].fillna(raw_data_copy['Loan_Amount'].median(),inplace=True)\n",
    "raw_data_copy['Loan_Amount_Term'].fillna(raw_data_copy['Loan_Amount_Term'].median(),inplace=True)\n",
    "\n",
    "validation_data_copy['Loan_Amount'].fillna(validation_data_copy['Loan_Amount'].median(),inplace=True)\n",
    "validation_data_copy['Loan_Amount_Term'].fillna(validation_data_copy['Loan_Amount_Term'].median(),inplace=True)\n",
    "\n",
    "#Check to see whether the missing values have been added\n",
    "print(f\"Number of Missing Values in raw_data_copy:\\n{raw_data_copy.isnull().sum()}\\n\")\n",
    "print(f\"Number of Missing Values in validation_data_copy:\\n{raw_data_copy.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handling Duplicates\n",
    "Having resolved the missing values in the datasets, we run the risk of producing duplicate records. To resolve this we need to identify and remove these duplicates. This can be done using various techniques such as the 'drop_duplicates' function in pandas, which removes duplicate rows based on all or selected columns. \n",
    "\n",
    "Removing duplicates is crucial as they can skew the results of the data analysis and lead to incorrect conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of duplicate rows in raw_data_copy: {raw_data_copy.duplicated().sum()}\")\n",
    "print(f\"Number of duplicate rows in validation_data_copy: {validation_data_copy.duplicated().sum()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are no duplicate records found in either of the datasets, no records need to be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Outlier Data Handling\n",
    "The next step to clean our data is to perform outlier handling. Outlier data handling can take place through the use of log transformations. Log transformations are beneficial as they reduce the impact of extreme values or outliers by compressing the scale of the data. \n",
    "\n",
    "This transformation also improves interpretability by expressing data in orders of magnitude, making large numbers more intuitive. Lastly, it’s particularly useful when the data is skewed, as it can transform the skewed data to approximate a normal distribution.\n",
    "\n",
    "#### ***Loan Amount***\n",
    "During the analysis process conducted on the datasets, it was noted that there was a partially normally distributed. Therefore, log transformation is required to normally distribute the data. A distribution graph and a box plot graph is created to visually see the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_copy['Loan_Amount_Log'] = np.log(raw_data_copy['Loan_Amount'])\n",
    "validation_data_copy['Loan_Amount_Log'] = np.log(validation_data_copy['Loan_Amount'])\n",
    "\n",
    "# Raw Data\n",
    "plt.figure(1, figsize=(8,5))\n",
    "plt.subplot(121)\n",
    "sns.distplot(raw_data_copy['Loan_Amount_Log'])\n",
    "plt.title('Log Transformed Loan Amount')\n",
    "plt.subplot(122)\n",
    "sns.boxplot(raw_data_copy['Loan_Amount_Log'])\n",
    "plt.title('Boxplot of Log Transformed Loan Amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Data\n",
    "plt.figure(1, figsize=(8,5))\n",
    "plt.subplot(121)\n",
    "sns.distplot(validation_data_copy['Loan_Amount_Log'])\n",
    "plt.title('Log Transformed Loan Amount')\n",
    "plt.subplot(122)\n",
    "sns.boxplot(validation_data_copy['Loan_Amount_Log'])\n",
    "plt.title('Boxplot of Log Transformed Loan Amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Transformation\n",
    "Data Transformation can be used to help improve the quality and reliability of the data, making it more suitable for data analysis. Furthermore, it can help in reducing data redundancy and improving data storage efficiency.\n",
    "\n",
    "#### ***Dependents***\n",
    "The *'Dependents'*  attribute values have the following categories: **(0, 1, 2, 3+}**. As you can see the first 3 options are numerical whereas the last ***\"3+\"*** is a string. This can cause issues during the building of our models, as logistic regression models only takes numerical values. To rectify this the value ***\"3+\"*** is replaced by the numerical value ***\"3\"***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_copy['Dependents'].replace('+3',3,inplace=True)\n",
    "validation_data_copy['Dependents'].replace('+3',3,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done this we can resolve the inconsistancy of the *'Dependents'* datatypes. In the raw_data dataset the attribute *'Dependents'* has the datatype ***\"object\"***, whereas in the validation_data dataset the attribute has a ***\"float64\"*** datatype. To resolve this the datatype of the attribute *'Dependents'* in validation_data_copy is converted to ***\"object\"*** to match the datatype in the raw_data dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_copy['Dependents'] = raw_data_copy['Dependents'].astype('object')\n",
    "print(f\"Dependents datatype: {raw_data_copy['Dependents'].dtypes}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Loan Status***\n",
    "The Loan_Status values ***\"Yes and No\"*** are replaced by ***\"1 and 0\"*** as we know that logistic regression models only take numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_copy['Loan_Status'].replace('N',0,inplace=True)\n",
    "raw_data_copy['Loan_Status'].replace('Y',1,inplace=True)\n",
    "\n",
    "#Check the replacement was successful\n",
    "print(f\"Values in {raw_data_copy['Loan_Status'].value_counts()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Remove attributes that do not affect the target attribute 'Loan_Status'\n",
    "\n",
    "Attributes that have no effect on the attribute ‘Loan_Status’ are removed. This will reduce the noise in the datasets as well as improve the efficiency of the data analysis process. By focusing only on the relevant attributes, we can prevent overfitting, enhance the interpretability of our models, and speed up the training process. Ultimately, this step helps to ensure that our predictive model is both accurate and robust.\n",
    "\n",
    "In both datasets the 'Loan_ID' attribute has no effect on the target attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_copy.drop('Loan_ID',axis=1,inplace=True)\n",
    "validation_data_copy.drop('Loan_ID',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Write the new datasets to CSV files\n",
    "The clean datasets are now written to csv files to be split and used during the model building phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_copy.to_csv('cleaned_raw_data.csv', index=False)\n",
    "validation_data_copy.to_csv('cleaned_validation_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================\n",
    "# **4. SPLIT THE RAW DATA INFORMATION**\n",
    "The use of dummy data is a common practice to transform categorical data into a binary format, specifically 0’s and 1’s. This transformation assists with  easier quantification and comparisons in future models. For instance, consider the *‘Gender’* attribute, which includes **‘Male’** and **‘Female’** categories. By employing the ‘dummies’ function from pandas, these categories are converted into binary form, where **‘Gender_Male’** is represented as **'1'** and **‘Gender_Female’** as **'0'**.\n",
    "\n",
    "When it comes to splitting the data into training and testing sets, a typical weightage of **80% (0.8)** is assigned to the *training* dataset, while the remaining **20% (0.2)** is allocated to the *testing* dataset. Additionaly, to ensure the consistency of the train/test split across multiple executions of the code, **‘random_state=42’** is used. This guarantees that the same train/test split is reproduced every time the code is run, thereby ensuring reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Cleaned CSV Files\n",
    "cleaned_raw_data = pd.read_csv('cleaned_raw_data.csv')\n",
    "cleaned_raw_data_copy = cleaned_raw_data.copy()\n",
    "\n",
    "# Define the independent variables (features) and the target variable\n",
    "X = cleaned_raw_data_copy.drop('Loan_Status', axis=1)  # all columns except 'Loan_Status'\n",
    "y = cleaned_raw_data_copy['Loan_Status']  # only 'Loan_Status' column\n",
    "\n",
    "# Convert categorical variable in the X dataset(all columns except 'Loan_Status') into dummy variables\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create new DataFrames for training and testing sets\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Save the training and testing sets to CSV files\n",
    "train_data.to_csv('train_data.csv', index=False)\n",
    "test_data.to_csv('test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================\n",
    "\n",
    "Now that we have our training and test data, we can move onto building the initial model.\n",
    "\n",
    "# **5. MODEL 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Build the Model\n",
    "A **Logistics Regression** model will be built using the train_data dataset and fit it to the model, whereas the test_data dataset will be used to predict the outcomes of the target attribute and compare the predictions to the actual answer to determin the accuracy of the model that was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Logistic Regression model and Fit the model with the training data\n",
    "model1 = LogisticRegression()\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the accuracy score for the predictions of the model\n",
    "test_predictions = model1.predict(X_test)\n",
    "print(f\"Accuracy Score for Predictions: {accuracy_score(y_test,test_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insight Gained:***\n",
    "\n",
    "    - The model shows it can accurately predict 77.24% of the Loan_Status values correctly.\n",
    "\n",
    "Cross validation will be used on the predictions generated by the model to check its validity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cross Validation model 1**\n",
    "Stratified K-Fold Cross Validation method is used. This variation of k-fold cross-validation is used when the target variable is imbalanced. It ensures that each fold is a good representative of the whole dataset. The average for all the iterated accuracy scores is calculated to determin the overall accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Stratified K-Fold Cross Validation\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "cross_val_predictions = cross_val_predict(model1, X, y, cv=skf)\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n",
    "i = 1\n",
    "scores = [] \n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    print('\\n{} of kfold {}'.format(i, kf.n_splits))\n",
    "    xtr, xvl = X.iloc[train_index], X.iloc[test_index]\n",
    "    ytr, yvl = y.iloc[train_index], y.iloc[test_index]\n",
    "    model = LogisticRegression(random_state=1)\n",
    "    model.fit(xtr, ytr)\n",
    "    pred_test = model.predict(xvl)\n",
    "    score = accuracy_score(yvl, pred_test)\n",
    "    scores.append(score)\n",
    "    print('accuracy_score:', score)\n",
    "    i += 1\n",
    "\n",
    "# Calculate the mean validation accuracy score\n",
    "mean_score = np.mean(scores)\n",
    "print(f\"\\nMean validation accuracy score: {mean_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Insight Gained:***\n",
    "\n",
    "    - The difference between the two accuracy scores can be attributed to the fact that cross-validation provides a \n",
    "      more robust measure of the model’s performance.\n",
    "    - In cross-validation, the model is trained and tested on different subsets of the data, which helps to ensure \n",
    "      that the model’s performance is not overly dependent on the specific way the data was split into training and\n",
    "      test sets.\n",
    "    - The higher mean validation accuracy score suggests that the model’s performance may be slightly better than what\n",
    "      was observed on the initial test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Predictions of the Model\n",
    "All the predictions generated from the first model are stored in CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the data from the Logistic Regression model Predictions.csv file to prevent duplicate storage\n",
    "open('Log_Reg_Mod1_Predictions.csv', 'w').close()\n",
    "open('Log_Reg_Mod1_Cross_Validate_Predictions.csv', 'w').close()\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df = pd.DataFrame(test_predictions, columns=['Predictions'])\n",
    "predictions_df.index.names = ['Index']\n",
    "predictions_df.to_csv('Log_Reg_Mod1_Predictions.csv', mode='a', header=True)\n",
    "\n",
    "# Save the cross-validation predictions to CSV file\n",
    "cross_val_predictions_df = pd.DataFrame(pred_test, columns=['Cross Validation Predictions'])\n",
    "cross_val_predictions_df.index.names = ['Index']\n",
    "cross_val_predictions_df.to_csv('Log_Reg_Mod1_Cross_Validate_Predictions.csv', mode='a', header=True)\n",
    "\n",
    "# Save the mean validation accuracy score to the same CSV file\n",
    "mean_score_df = pd.DataFrame([mean_score], columns=['Mean Validation Accuracy Score'])\n",
    "mean_score_df.to_csv('Log_Reg_Mod1_Cross_Validate_Predictions.csv', mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Feature Importance from the Model\n",
    "Feature engineering transforms or combines raw data into a format that can be easily understood by machine learning models.\n",
    "Creates predictive model features, also known as a dimensions or variables, to generate model predictions.\n",
    "This highlights the most important patterns and relationships in the data, which then assists the machine learning model to learn from the data more effectively.\n",
    "\n",
    "#### **Feature 1: Total Income**\n",
    "*'Total_Income'* is the first feature that can be created. It is achieved through the addition of the *'Applicants_Income'* and the *'Coapplicant_Income'*. The Total_Income is then normalised to reduce the affects of the extreme values that could arise from the addition of the two attributes. A distribution chart is created to visually see the new feature and its distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Total_Income']=train_data['Applicant_Income']+train_data['Coapplicant_Income']\n",
    "test_data['Total_Income']=test_data['Applicant_Income']+test_data['Coapplicant_Income']\n",
    "\n",
    "#Distribution normalization\n",
    "sns.distplot(train_data['Total_Income'])\n",
    "plt.title('Distribution of Total Income')\n",
    "plt.xlabel('Total Income')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "train_data['Total_Income_Log']=np.log(train_data['Total_Income'])\n",
    "test_data['Total_Income_Log']=np.log(test_data['Total_Income'])\n",
    "\n",
    "sns.distplot(train_data['Total_Income_Log'])\n",
    "plt.title('Distribution of Total Income Log')\n",
    "plt.xlabel('Total Income Log')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Feature 2: Equated Monthly Installment (EMI)**\n",
    "The second feature we can create is an *'EMI'* attribute. It can be created by dividing the *‘Loan_Amount’* by the *‘Loan_Amount_Term’*. This feature gets the monthly payment amount for a loan, given the total loan amount and the term of the loan. Overall this will give an indication of the individuals monthly financial obligation towards the loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['EMI']=train_data['Loan_Amount']/train_data['Loan_Amount_Term']\n",
    "test_data['EMI'] = test_data['Loan_Amount']/test_data['Loan_Amount_Term']\n",
    "\n",
    "sns.distplot(train_data['EMI'])\n",
    "plt.title('Distribution of Equated Monthly Installments')\n",
    "plt.xlabel('Equated Monthly Installment')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Feature 3: Income After EMI**\n",
    "Lastly, a feature called \"Income After EMI\" can be created by dividing the ‘Loan_Amount’ by the ‘Loan_Amount_Term’ to get the monthly payment amount for a loan. This will give an indication of the individuals monthly financial obligation towards the loan. The 'EMI' feature is multiplied with 1000 to make the unit equal to the 'Total_Income' unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature 3: Income After EMI\n",
    "train_data['Income_After_EMI']=train_data['Total_Income']-(train_data['EMI']*1000)\n",
    "test_data['Income_After_EMI']=test_data['Total_Income']-(test_data['EMI']*1000)\n",
    "\n",
    "sns.distplot(train_data['Income_After_EMI'])\n",
    "plt.title('Distribution of Income After EMI')\n",
    "plt.xlabel('Income After EMI')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Remove all features that created the new features**\n",
    "The last step in the feature engineering section is to remove all the attributes used to create the new features. This is due to the high correlation between those old attributes and the new features. A Logistic regression model assumes that the attributes are not highly correlated. Therefore any excess noise in the datasets are removed. The new features are stored into a CSV file for use in the second model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.drop(['Applicant_Income','Coapplicant_Income','Loan_Amount','Loan_Amount_Term'],axis=1)\n",
    "test_data=test_data.drop(['Applicant_Income','Coapplicant_Income','Loan_Amount','Loan_Amount_Term'],axis=1)\n",
    "\n",
    "#Check to see if the attributes have been removed\n",
    "print(f\"Training Data Columns: {train_data.columns}\\n\")\n",
    "print(f\"Testing Data Columns:{test_data.columns}\\n\")\n",
    "\n",
    "# Store new Features in CSV files\n",
    "train_data.to_csv('Feature_Importance_train_data_NF_Model1.csv', index=False)\n",
    "test_data.to_csv('Feature_Importance_test_data_NF_Model1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Create Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a pickle file\n",
    "with open('Model_1.pkl', 'wb') as f:\n",
    "    pickle.dump(model1, f)\n",
    "\n",
    "#View data in the Model_1.pkl file\n",
    "with open('Model_1.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(f\"Model 1 pickle file data: {data}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================\n",
    "# **5. MODEL 2**\n",
    "\n",
    "## A. Build the Model\n",
    "\n",
    "## B. Predictions of the Model\n",
    "\n",
    "## C. Feature Importance from the Model\n",
    "#### **Feature 1: Total Income**\n",
    "\n",
    "## D. Create Pickle File"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
